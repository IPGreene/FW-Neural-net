{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20190916200805-0000\n",
      "KERNEL_ID = 2e511fdf-e281-4554-a8aa-9781373c3f23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sourcePort='52217', destinationPort='2000', protocolName='tcp_ip', IPgeo=None, deviceId='31410', event_category='5010', categoryDescription='Misc Exploit', eventDescription='Unrecognized Palo Alto PA Series Vulnerability Exploit Threat Event', relevance='8', credibility='10', severity='9', magnitude='9', Event_DateTime='Mar 14, 2019, 8:26:53 PM', eventCount='1'),\n",
       " Row(sourcePort='51405', destinationPort='80', protocolName='tcp_ip', IPgeo=None, deviceId='31410', event_category='7024', categoryDescription='Information Leak', eventDescription='A directory traversal vulnerability has been discovered in parsing malformed HTTP requests. This vulnerability is due to a lack of proper checks in HTTP URI requests. A successful attack could result in access to sensitive information that could further aid in other attacks.', relevance='8', credibility='10', severity='1', magnitude='6', Event_DateTime='Mar 14, 2019, 8:26:53 PM', eventCount='1'),\n",
       " Row(sourcePort='36002', destinationPort='445', protocolName='tcp_ip', IPgeo=None, deviceId='31410', event_category='7024', categoryDescription='Information Leak', eventDescription='This alert indicates an attempt to read a windows registry remotely has been detected.', relevance='8', credibility='10', severity='1', magnitude='6', Event_DateTime='Mar 14, 2019, 8:26:52 PM', eventCount='1'),\n",
       " Row(sourcePort='35074', destinationPort='445', protocolName='tcp_ip', IPgeo=None, deviceId='31410', event_category='7024', categoryDescription='Information Leak', eventDescription='This alert indicates an attempt to read a windows registry remotely has been detected.', relevance='8', credibility='10', severity='1', magnitude='6', Event_DateTime='Mar 14, 2019, 8:26:46 PM', eventCount='1'),\n",
       " Row(sourcePort='55631', destinationPort='443', protocolName='tcp_ip', IPgeo='United States', deviceId='31410', event_category='4002', categoryDescription='Firewall Permit', eventDescription='Session was allowed by policy', relevance='10', credibility='10', severity='0', magnitude='6', Event_DateTime='Mar 14, 2019, 8:26:54 PM', eventCount='66')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-189ca005-d852-4370-b4dd-50a8499acec6',\n",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'api_key': 'mAo40B3h-s0GPQnsakluqLLmGPlQkbNmHif2xzcvDY6C'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_9bed24a8267a4d0ea5db2140346dee9d_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data_1 = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('ASA_log.csv', 'capstoneproject-donotdelete-pr-68gfgxkh2rslzx'))\n",
    "df_data_1.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+-------------+--------+--------------+-------------------+--------------------+---------+-----------+--------+---------+----------+\n",
      "|sourcePort|destinationPort|protocolName|        IPgeo|deviceId|event_category|categoryDescription|    eventDescription|relevance|credibility|severity|magnitude|eventCount|\n",
      "+----------+---------------+------------+-------------+--------+--------------+-------------------+--------------------+---------+-----------+--------+---------+----------+\n",
      "|     52217|           2000|      tcp_ip|      Unknown|   31410|          5010|       Misc Exploit|Unrecognized Palo...|        8|         10|       9|        9|         1|\n",
      "|     51405|             80|      tcp_ip|      Unknown|   31410|          7024|   Information Leak|A directory trave...|        8|         10|       1|        6|         1|\n",
      "|     36002|            445|      tcp_ip|      Unknown|   31410|          7024|   Information Leak|This alert indica...|        8|         10|       1|        6|         1|\n",
      "|     35074|            445|      tcp_ip|      Unknown|   31410|          7024|   Information Leak|This alert indica...|        8|         10|       1|        6|         1|\n",
      "|     55631|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|        66|\n",
      "|     55991|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|        39|\n",
      "|     49792|             80|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         9|\n",
      "|     64121|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         4|\n",
      "|     51662|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         3|\n",
      "|     50545|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         2|\n",
      "|     65255|             80|      tcp_ip|United States|     507|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "|     65025|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "|     64920|            443|      tcp_ip|United States|     507|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "|     64680|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "|     64674|             80|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "|     64672|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "|     64670|             80|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "|     64668|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "|     64576|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "|     64533|            443|      tcp_ip|United States|   31410|          4002|    Firewall Permit|Session was allow...|       10|         10|       0|        6|         1|\n",
      "+----------+---------------+------------+-------------+--------+--------------+-------------------+--------------------+---------+-----------+--------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# Use spark.sql to create a pyspark dataframe\n",
    "# cast data from stringtype to integertype where appropriate\n",
    "# data is either an integer or string\n",
    "\n",
    "df_data_1.createOrReplaceTempView(\"firewall\")\n",
    "data = spark.sql('SELECT * FROM firewall')\n",
    "data = data.withColumn('sourcePort', data['sourcePort'].cast(IntegerType()))\n",
    "data = data.withColumn('destinationPort', data['destinationPort'].cast(IntegerType()))\n",
    "data = data.withColumn('deviceId', data['deviceId'].cast(IntegerType()))\n",
    "data = data.withColumn('event_category', data['event_category'].cast(IntegerType()))\n",
    "data = data.withColumn('relevance', data['relevance'].cast(IntegerType()))\n",
    "data = data.withColumn('credibility', data['credibility'].cast(IntegerType()))\n",
    "data = data.withColumn('severity', data['severity'].cast(IntegerType()))\n",
    "data = data.withColumn('magnitude', data['magnitude'].cast(IntegerType()))\n",
    "data = data.withColumn('eventCount', data['eventCount'].cast(IntegerType()))\n",
    "data = data.drop('Event_DateTime')\n",
    "data = data.fillna('Unknown')\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we change string columns to indexed columns with string indexer\n",
    "# then we onehotencode those columns and use a vector assembler to create a features column for them\n",
    "categorical_columns = ['protocolName', 'IPgeo', 'event_category','eventDescription', 'categoryDescription']\n",
    "output_columns = ['pnlbl', 'ipglbl', 'evcatlbl', 'ecdlbl', 'catdeslbl']\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c), handleInvalid=\"keep\")\n",
    "    for c in categorical_columns\n",
    "]\n",
    "\n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=indexer.getOutputCol(), outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "           for indexer in indexers\n",
    "          ]\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders], outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "model=pipeline.fit(data)\n",
    "transformed = model.transform(data)\n",
    "dataindex = transformed\n",
    "# drop the original columns before being transformed, I'm unsure if this is correct but\n",
    "# the original column types can't be passed into keras\n",
    "for col in categorical_columns:\n",
    "    dataindex = dataindex.drop(col)\n",
    "x = dataindex.drop('features')\n",
    "y = dataindex[['features']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c531159df6f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m  \u001b[0;34m'protocolName_indexed_encoded'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m  \u001b[0;34m'IPgeo_indexed_encoded'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m  'event_category_indexed_encoded',).collect())\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# try to change the datafram to a numpy array so it can be fed into keras\n",
    "x_np = np.array(x.select('sourcePort',\n",
    " 'destinationPort',\n",
    " 'deviceId',\n",
    " 'relevance',\n",
    " 'credibility',\n",
    " 'severity',\n",
    " 'magnitude',\n",
    " 'eventCount',\n",
    " 'protocolName_indexed',\n",
    " 'IPgeo_indexed',\n",
    " 'event_category_indexed',\n",
    " 'protocolName_indexed_encoded',\n",
    " 'IPgeo_indexed_encoded',\n",
    " 'event_category_indexed_encoded',).collect())\n",
    "# wont acept a sequence as an array value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was the setup for the neural net but as I haven't been able to pass any of the data into it in\n",
    "# the correct format I haven't updated it yet\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#get number of columns in training data\n",
    "notnneded, n_cols_2 =xtrain_np.shape \n",
    "\n",
    "#add layers to model\n",
    "model.add(Dense(250, activation='relu', input_shape=(n_cols_2,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(xtrainnp, ytrainnp, epochs=10, batch_size=64)\n",
    "score = model.evaluate(xtestnp, ytestnp, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
