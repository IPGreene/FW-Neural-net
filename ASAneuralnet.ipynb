{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20190916183034-0001\n",
      "KERNEL_ID = eaeb136b-920d-47c3-95f9-cbd1f0edd06d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sourcePort='52217', destinationPort='2000', protocolName='tcp_ip', IPgeo=None, deviceId='31410', event_category='5010', categoryDescription='Misc Exploit', eventDescription='Unrecognized Palo Alto PA Series Vulnerability Exploit Threat Event', relevance='8', credibility='10', severity='9', magnitude='9', Event_DateTime='Mar 14, 2019, 8:26:53 PM', eventCount='1'),\n",
       " Row(sourcePort='51405', destinationPort='80', protocolName='tcp_ip', IPgeo=None, deviceId='31410', event_category='7024', categoryDescription='Information Leak', eventDescription='A directory traversal vulnerability has been discovered in parsing malformed HTTP requests. This vulnerability is due to a lack of proper checks in HTTP URI requests. A successful attack could result in access to sensitive information that could further aid in other attacks.', relevance='8', credibility='10', severity='1', magnitude='6', Event_DateTime='Mar 14, 2019, 8:26:53 PM', eventCount='1'),\n",
       " Row(sourcePort='36002', destinationPort='445', protocolName='tcp_ip', IPgeo=None, deviceId='31410', event_category='7024', categoryDescription='Information Leak', eventDescription='This alert indicates an attempt to read a windows registry remotely has been detected.', relevance='8', credibility='10', severity='1', magnitude='6', Event_DateTime='Mar 14, 2019, 8:26:52 PM', eventCount='1'),\n",
       " Row(sourcePort='35074', destinationPort='445', protocolName='tcp_ip', IPgeo=None, deviceId='31410', event_category='7024', categoryDescription='Information Leak', eventDescription='This alert indicates an attempt to read a windows registry remotely has been detected.', relevance='8', credibility='10', severity='1', magnitude='6', Event_DateTime='Mar 14, 2019, 8:26:46 PM', eventCount='1'),\n",
       " Row(sourcePort='55631', destinationPort='443', protocolName='tcp_ip', IPgeo='United States', deviceId='31410', event_category='4002', categoryDescription='Firewall Permit', eventDescription='Session was allowed by policy', relevance='10', credibility='10', severity='0', magnitude='6', Event_DateTime='Mar 14, 2019, 8:26:54 PM', eventCount='66')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-189ca005-d852-4370-b4dd-50a8499acec6',\n",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'api_key': 'mAo40B3h-s0GPQnsakluqLLmGPlQkbNmHif2xzcvDY6C'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_9bed24a8267a4d0ea5db2140346dee9d_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data_1 = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('ASA_log.csv', 'capstoneproject-donotdelete-pr-68gfgxkh2rslzx'))\n",
    "df_data_1.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, Normalizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_data_1.createOrReplaceTempView(\"firewall\")\n",
    "data = spark.sql('SELECT * FROM firewall')\n",
    "\n",
    "#for col, t in data.dtypes:\n",
    "#    if t == 'string':\n",
    "#        try:\n",
    "#            data = data.withColumn(col, data[col].cast(IntegerType()))\n",
    "#        except:\n",
    "#            pass\n",
    "#data = data.drop('Event_DateTime')        \n",
    " \n",
    "data = data.withColumn('sourcePort', data['sourcePort'].cast(IntegerType()))\n",
    "data = data.withColumn('destinationPort', data['destinationPort'].cast(IntegerType()))\n",
    "data = data.withColumn('deviceId', data['deviceId'].cast(IntegerType()))\n",
    "data = data.withColumn('event_category', data['event_category'].cast(IntegerType()))\n",
    "data = data.withColumn('relevance', data['relevance'].cast(IntegerType()))\n",
    "data = data.withColumn('credibility', data['credibility'].cast(IntegerType()))\n",
    "data = data.withColumn('severity', data['severity'].cast(IntegerType()))\n",
    "data = data.withColumn('magnitude', data['magnitude'].cast(IntegerType()))\n",
    "data = data.withColumn('eventCount', data['eventCount'].cast(IntegerType()))\n",
    "data = data.drop('Event_DateTime')\n",
    "data = data.fillna('Unknown')\n",
    "#x = data.drop('magnitude')\n",
    "#y = data[['magnitude']]\n",
    "#xtrain, xtest = x.randomSplit([0.66, 0.34])\n",
    "#ytrain, ytest = y.randomSplit([0.66, 0.34])\n",
    "\n",
    "#data.printSchema()\n",
    "#print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sourcePort',\n",
       " 'destinationPort',\n",
       " 'deviceId',\n",
       " 'relevance',\n",
       " 'credibility',\n",
       " 'severity',\n",
       " 'magnitude',\n",
       " 'eventCount',\n",
       " 'protocolName_indexed',\n",
       " 'IPgeo_indexed',\n",
       " 'event_category_indexed',\n",
       " 'eventDescription_indexed',\n",
       " 'categoryDescription_indexed',\n",
       " 'protocolName_indexed_encoded',\n",
       " 'IPgeo_indexed_encoded',\n",
       " 'event_category_indexed_encoded',\n",
       " 'eventDescription_indexed_encoded',\n",
       " 'categoryDescription_indexed_encoded']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer\n",
    "#\"{0}_indexed\".format(c)\n",
    "#x_train.show()\n",
    "categorical_columns = ['protocolName', 'IPgeo', 'event_category','eventDescription', 'categoryDescription']\n",
    "output_columns = ['pnlbl', 'ipglbl', 'evcatlbl', 'ecdlbl', 'catdeslbl']\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c), handleInvalid=\"keep\")\n",
    "    for c in categorical_columns\n",
    "]\n",
    "\n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=indexer.getOutputCol(), outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "           for indexer in indexers\n",
    "          ]\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders], outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "model=pipeline.fit(data)\n",
    "transformed = model.transform(data)\n",
    "dataindex = transformed\n",
    "for col in categorical_columns:\n",
    "    dataindex = dataindex.drop(col)\n",
    "x = dataindex.drop('features')\n",
    "y = dataindex[['features']]\n",
    "x.columns\n",
    "#x.show()\n",
    "#y.show()\n",
    "#transformed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['sourcePort',\n",
    " 'destinationPort',\n",
    " 'deviceId',\n",
    " 'relevance',\n",
    " 'credibility',\n",
    " 'severity',\n",
    " 'magnitude',\n",
    " 'eventCount',\n",
    " 'protocolName_indexed',\n",
    " 'IPgeo_indexed',\n",
    " 'event_category_indexed',\n",
    " 'eventDescription_indexed',\n",
    " 'categoryDescription_indexed',\n",
    " 'protocolName_indexed_encoded',\n",
    " 'IPgeo_indexed_encoded',\n",
    " 'event_category_indexed_encoded',\n",
    " 'eventDescription_indexed_encoded',\n",
    " 'categoryDescription_indexed_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-a04833fd38ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m  \u001b[0;34m'protocolName_indexed_encoded'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m  \u001b[0;34m'IPgeo_indexed_encoded'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m  'event_category_indexed_encoded',).collect())\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#xtestnp = np.array(x.select('sourcePort',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 'destinationPort',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#datapipe.toPandas()\n",
    "#datapipe1 = datapipe.toPandas()\n",
    "#print(datapipe1)\n",
    "import numpy as np\n",
    "\n",
    "x_np = np.array(x.select('sourcePort',\n",
    " 'destinationPort',\n",
    " 'deviceId',\n",
    " 'relevance',\n",
    " 'credibility',\n",
    " 'severity',\n",
    " 'magnitude',\n",
    " 'eventCount',\n",
    " 'protocolName_indexed',\n",
    " 'IPgeo_indexed',\n",
    " 'event_category_indexed',\n",
    " 'protocolName_indexed_encoded',\n",
    " 'IPgeo_indexed_encoded',\n",
    " 'event_category_indexed_encoded',).collect())\n",
    "#xtestnp = np.array(x.select('sourcePort',\n",
    "# 'destinationPort',\n",
    "# 'deviceId',\n",
    "# 'relevance',\n",
    "# 'credibility',\n",
    "# 'severity',\n",
    "# 'magnitude',\n",
    "# 'eventCount',\n",
    "# 'protocolName_indexed',\n",
    "# 'IPgeo_indexed',\n",
    "# 'event_category_indexed',\n",
    "# 'categoryDescription_indexed',\n",
    "# 'eventDescription_indexed',\n",
    "# 'protocolName_indexed_encoded',\n",
    "# 'IPgeo_indexed_encoded',\n",
    "# 'event_category_indexed_encoded',\n",
    "# 'categoryDescription_indexed_encoded',\n",
    "# 'eventDescription_indexed_encoded').collect())\n",
    "y_np = np.array(y.select('features').collect())\n",
    "#ytestnp = np.array(y1test.select('magnitude_index').collect())\n",
    "#print(xtrainnp.shape)\n",
    "#print(xtestnp.shape)\n",
    "#print(ytrainnp.shape)\n",
    "#print(ytestnp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general summary statistcs are meaningless on this data. box plots of counts for some of the ports might be but graphically this data\n",
    "# has very little significance\n",
    "#for col in xtrain.columns:\n",
    "#    xtrain.describe(col).show()\n",
    "#\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StringIndexerModel' object has no attribute 'getOutputCol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-79f5d79e42b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoderEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_vec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mstages\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StringIndexerModel' object has no attribute 'getOutputCol'"
     ]
    }
   ],
   "source": [
    "#cat_features = data.columns\n",
    "#stages = []\n",
    "#for feature in data.columns:\n",
    "#    indexers = StringIndexer(inputCol=features, outputCol=features+'_index').fit(data)\n",
    "#    encoder = OneHotEncoderEstimator(inputCol=[indexers.getOutputCol()], outputCol=[features+'_vec'])\n",
    "#    stages += [indexers, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3a9fb25a8796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mindexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mencoders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoderEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'magnitude_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'magnitude_vec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "#n_cols = len(x.columns)\n",
    "#from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\").fit(data) \n",
    "    for column in data.columns\n",
    "]\n",
    "\n",
    "indexed = indexers.transform(data)\n",
    "encoders = OneHotEncoderEstimator(inputCols=['magnitude_index'], outputCols=['magnitude_vec']) \n",
    "encoded = encoders.transform(indexed)\n",
    "#pipeline = Pipeline(stages=[indexers, encoders])\n",
    "#data_r = pipeline.fit(data)\n",
    "#transformed = data_r.transform(data)\n",
    "#dataindex = data_r\n",
    "#for col in data.columns:\n",
    "#    dataindex = dataindex.drop(col)\n",
    "#dataindex.show()\n",
    "#x1 = dataindex.drop('magnitude_index')\n",
    "#y1 = dataindex[['magnitude_index']]\n",
    "#dataindex.columns\n",
    "#x1train, x1test = x1.randomSplit([0.66, 0.34])\n",
    "#y1train, y1test = y1.randomSplit([0.66, 0.34])\n",
    "#x1train.columns\n",
    "#numpycol = ['sourcePort_index',\n",
    "# 'destinationPort_index',\n",
    "# 'protocolName_index',\n",
    "# 'IPgeo_index',\n",
    "# 'deviceId_index',\n",
    "# 'event_category_index',\n",
    "# 'categoryDescription_index',\n",
    "# 'eventDescription_index',\n",
    "# 'relevance_index',\n",
    "# 'credibility_index',\n",
    "# 'severity_index',\n",
    "# 'eventCount_index']\n",
    "\n",
    "#x1train = np.array()\n",
    "#x1train.show()\n",
    "#y1train.show()\n",
    "\n",
    "\n",
    "\n",
    "#pipeline = Pipeline(stages=encoders)\n",
    "#datapipe = pipeline.fit(data_r).transform(data_r)\n",
    "#datapipe = data_r\n",
    "#for col in data.columns:\n",
    "#    datapipe = datapipe.drop(col)\n",
    "#datapipe.show()\n",
    "\n",
    "#for col in data_r.columns:\n",
    "#    data_x = data_x.drop(col)\n",
    "#encoders = [\n",
    "#    OneHotEncoder(inputCol=column, outputCol=column+\"_vec\") \n",
    "#    for column in y1.columns\n",
    "#]\n",
    "#pipeline = Pipeline(stages=encoders)\n",
    "#data_y = pipeline.fit(y1).transform(y1)\n",
    "#for col in data_r.columns:\n",
    "#    data_y = data_y.drop(col)\n",
    "#data_x.printSchema()\n",
    "#data_y.printSchema()\n",
    "#data_xtrain, data_xtest = data_x.randomSplit([0.66, 0.34])\n",
    "#data_ytrain, data_ytest = data_y.randomSplit([0.66, 0.34])\n",
    "#print(type(data_xtrain))\n",
    "\n",
    "#print(data_xtrain)\n",
    "\n",
    "#indexer = StringIndexer(inputCol = 'sourcePort', outputCol = 'scplbl')\n",
    "#encoder = OneHotEncoder(inputCol = 'scplbl', outputCol='scplblvec')\n",
    "#pipeline = Pipeline(stages=[indexer, encoder])\n",
    "#model=pipeline.fit(data)\n",
    "#transformed = model.transform(data)\n",
    "#transformed.show()\n",
    "#data.show()\n",
    "\n",
    "#def string_to_float(x):\n",
    "#    return float(x)\n",
    "#udfstring_to_float = udf(string_to_float, StringType())\n",
    "#data.withColumn('sourcePort', udfstring_to_float(\"numberfloat\"))\n",
    "#data.printSchema()\n",
    "#indexer = StringIndexer(inputCol = 'sourcePort', outputCol = 'scplbl')\n",
    "#encoder = OneHoteEncoder(inputCol = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c45c545f6511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m  \u001b[0;34m'magnitude_index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m  'eventCount_index').collect())\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mytrainnp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagnitude_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mis\u001b[0m \u001b[0mplaced\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#from keras.utils import to_categorical\n",
    "dataindex.columns\n",
    "dataindexnp = np.array(dataindex.select('sourcePort_index',\n",
    " 'destinationPort_index',\n",
    " 'protocolName_index',\n",
    " 'IPgeo_index',\n",
    " 'deviceId_index',\n",
    " 'event_category_index',\n",
    " 'categoryDescription_index',\n",
    " 'eventDescription_index',\n",
    " 'relevance_index',\n",
    " 'credibility_index',\n",
    " 'severity_index',\n",
    " 'magnitude_index',\n",
    " 'eventCount_index').collect())\n",
    "ytrainnp = to_categorical(dataindex.magnitude_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_12 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d8f161fcc0e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrainnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrainnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtestnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytestnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_12 to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#get number of columns in training data\n",
    "notnneded, n_cols_2 =xtrainnp.shape \n",
    "\n",
    "#add layers to model\n",
    "model.add(Dense(250, activation='relu', input_shape=(n_cols_2,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(xtrainnp, ytrainnp, epochs=10, batch_size=64)\n",
    "score = model.evaluate(xtestnp, ytestnp, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer\n",
    "#from pyspark.sql.functions import udf\n",
    "#from pyspark.sql.types import *\n",
    "# All the code below to the double space works for single column\n",
    "#data = df_data_1.select('sourcePort', 'IPgeo', 'protocolName')\n",
    "#data = data.fillna('Unknown')\n",
    "#data.show()\n",
    "#data = data.withColumn('sourcePort', data['sourcePort'].cast(IntegerType()))\n",
    "#This code works\n",
    "#indexer = StringIndexer(inputCol = 'IPgeo', outputCol = 'ipglbl')\n",
    "#indexed_df = indexer.fit(data).transform(data)\n",
    "#encoder = OneHotEncoder(inputCol = 'ipglbl', outputCol='ipglblvec')\n",
    "#encoded_df = encoder.transform(indexed_df)\n",
    "#encoded_df.show()\n",
    "\n",
    "#indexer = StringIndexer(inputCol = 'sourcePort', outputCol = 'scplbl')\n",
    "#encoder = OneHotEncoder(inputCol = 'scplbl', outputCol='scplblvec')\n",
    "#pipeline = Pipeline(stages=[indexer, encoder])\n",
    "#model=pipeline.fit(data)\n",
    "#transformed = model.transform(data)\n",
    "#transformed.show()\n",
    "#data.show()\n",
    "\n",
    "#def string_to_float(x):\n",
    "#    return float(x)\n",
    "#udfstring_to_float = udf(string_to_float, StringType())\n",
    "#data.withColumn('sourcePort', udfstring_to_float(\"numberfloat\"))\n",
    "#data.printSchema()\n",
    "#indexer = StringIndexer(inputCol = 'sourcePort', outputCol = 'scplbl')\n",
    "#encoder = OneHoteEncoder(inputCol = )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
